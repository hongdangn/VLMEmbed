python train_distillation.py --model_name llava-hf/llava-onevision-qwen2-0.5b-ov-hf --lora True --lora_r 8 --model_backbone llava_onevision --pooling eos --dataset_name TIGER-Lab/MMEB-train --subset_name ImageNet_1K N24News HatefulMemes VOC2007 SUN397 --dataset_split original --image_dir vlm2vec_train/MMEB-train --output_dir training/distill_B2_Qwen2_2B --per_device_train_batch_size 8 --gradient_accumulation_steps 4 --learning_rate 1e-5 --num_train_epochs 1 --bf16 --save_total_limit 2 --logging_steps 1 --save_strategy epoch --seed 42 --weight_decay 0.01  --normalize True --lr_scheduler_type cosine --warmup_ratio 0.03 --bf16 True --image_resolution low --report_to wandb --kd_weight 0.3
# Fix image_resolution to low to feed VRAM